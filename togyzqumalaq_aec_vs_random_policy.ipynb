{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "luAZ42vu0ipC",
        "1aNj2Mfm0Zb-",
        "i9JzOFeK1kmm",
        "46_0KK8E1rWM",
        "nF0ZMW3wLv-B",
        "T7IIe3E9M69q",
        "sNPVNMBTNW3z",
        "b9q8lRmx-FU_",
        "ARZK7tZNb4J3",
        "pIAdRMbp2zS-"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGhcCfeL+arnGehJgaNyQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhus-dika/togyz-qumalaq-agent/blob/main/togyzqumalaq_aec_vs_random_policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ðŸ˜ AEC environment https://pettingzoo.farama.org/api/aec/#about-aec"
      ],
      "metadata": {
        "id": "n-ODLQYxcX21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  ðŸ¦œ Install need packages"
      ],
      "metadata": {
        "id": "luAZ42vu0ipC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install pettingzoo[classic]==1.23.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXSpI1zG0l9u",
        "outputId": "c33f09a8-6536-4250-bca1-9025c88f0354"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting pettingzoo[classic]==1.23.0\n",
            "  Downloading pettingzoo-1.23.0-py3-none-any.whl (826 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m826.8/826.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[classic]==1.23.0) (1.25.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[classic]==1.23.0) (0.29.1)\n",
            "Collecting chess==1.7.0 (from pettingzoo[classic]==1.23.0)\n",
            "  Downloading chess-1.7.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.1/147.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rlcard==1.0.5 (from pettingzoo[classic]==1.23.0)\n",
            "  Downloading rlcard-1.0.5.tar.gz (251 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.1/251.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.3.0 (from pettingzoo[classic]==1.23.0)\n",
            "  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hanabi-learning-environment==0.0.4 (from pettingzoo[classic]==1.23.0)\n",
            "  Downloading hanabi_learning_environment-0.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (115 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from hanabi-learning-environment==0.0.4->pettingzoo[classic]==1.23.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from rlcard==1.0.5->pettingzoo[classic]==1.23.0) (2.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[classic]==1.23.0) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[classic]==1.23.0) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[classic]==1.23.0) (0.0.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->hanabi-learning-environment==0.0.4->pettingzoo[classic]==1.23.0) (2.22)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.0.5-py3-none-any.whl size=307097 sha256=765aac62eb17e4c04bdd329818b282aa9d86ce90f5cb3d727a7b7bf4f0781f49\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/1b/a9/cb9a2009aefc3e311979e3764bc8e095cc6a2541fdc3ce1f60\n",
            "Successfully built rlcard\n",
            "Installing collected packages: rlcard, pygame, chess, pettingzoo, hanabi-learning-environment\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed chess-1.7.0 hanabi-learning-environment-0.0.4 pettingzoo-1.23.0 pygame-2.3.0 rlcard-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸž Imports"
      ],
      "metadata": {
        "id": "1aNj2Mfm0Zb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "from gymnasium.spaces import Discrete, MultiDiscrete\n",
        "from gymnasium import spaces\n",
        "\n",
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils import agent_selector, wrappers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "NUM_ITERS = 400\n",
        "PLAYS = {\"bastaushy\": 0, \"qostaushy\": 0}"
      ],
      "metadata": {
        "id": "vaX0bAwy0cd5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ¦‰ Create environment"
      ],
      "metadata": {
        "id": "i9JzOFeK1kmm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "S3Tq7OaYcUfq"
      },
      "outputs": [],
      "source": [
        "def env(render_mode=None):\n",
        "    \"\"\"\n",
        "    The env function often wraps the environment in wrappers by default.\n",
        "    You can find full documentation for these methods\n",
        "    elsewhere in the developer documentation.\n",
        "    \"\"\"\n",
        "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
        "    env = raw_env(render_mode=internal_render_mode)\n",
        "    # This wrapper is only for environments which print results to the terminal\n",
        "    if render_mode == \"ansi\":\n",
        "        env = wrappers.CaptureStdoutWrapper(env)\n",
        "    # this wrapper helps error handling for discrete action spaces\n",
        "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
        "    # Provides a wide vareity of helpful user errors\n",
        "    # Strongly recommended\n",
        "    env = wrappers.OrderEnforcingWrapper(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "class raw_env(AECEnv):\n",
        "    \"\"\"\n",
        "    The metadata holds environment constants. From gymnasium, we inherit the \"render_modes\",\n",
        "    metadata which specifies which modes can be put into the render() method.\n",
        "    At least human mode should be supported.\n",
        "    The \"name\" metadata allows the environment to be pretty printed.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"ansi\", \"human\"],\n",
        "        \"name\": \"togyzqumalaq_v0\"\n",
        "        }\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        \"\"\"\n",
        "        The init method takes in environment arguments and\n",
        "         should define the following attributes:\n",
        "        - otaular\n",
        "        - tuzdyq\n",
        "        - qazandar\n",
        "        - possible_agents\n",
        "        - render_mode\n",
        "\n",
        "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
        "        Spaces should be defined in the action_space() and observation_space() methods.\n",
        "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
        "\n",
        "        These attributes should not be changed after initialization.\n",
        "        \"\"\"\n",
        "        self.otaular = []\n",
        "        self.tuzdyq = []\n",
        "        self.qazandar = []\n",
        "        self.direction = []\n",
        "        self.agents = [\"bastaushy\", \"qostaushy\"]\n",
        "        self.possible_agents = self.agents[:]\n",
        "        # optional: we can define the observation and action spaces here as attributes to be used in their corresponding methods\n",
        "        self.action_spaces = {i: spaces.Discrete(9) for i in self.agents}\n",
        "        self.observation_spaces = {\n",
        "            i: spaces.Dict(\n",
        "                {\n",
        "                    \"observation\": MultiDiscrete([100] * 18 + [9] * 2 + [82] * 2),\n",
        "                    \"action_mask\": Discrete(9),\n",
        "                }\n",
        "            )\n",
        "            for i in self.agents\n",
        "        }\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    # Observation space should be defined here.\n",
        "    def action_space(self, agent):\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    # Action space should be defined here.\n",
        "    def observation_space(self, agent):\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Renders the environment. In human mode, it can print to terminal, open\n",
        "        up a graphical window, or open up some other display that a human can see and understand.\n",
        "        \"\"\"\n",
        "        \"\"\"Renders the environment.\"\"\"\n",
        "        if self.render_mode is None:\n",
        "            gymnasium.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if len(self.agents) == 2:\n",
        "            points_bastaushy_x = np.array([i * 2 for i in range(10)])\n",
        "            points_bastaushy_y = np.array([i % 5 for i in range(50)])\n",
        "\n",
        "            x = np.arange(-3, 225, 1)\n",
        "            y = -1\n",
        "\n",
        "            text_kwargs = dict(ha='center', va='center', fontsize=12)\n",
        "            plt.figure(figsize=(17, 6))\n",
        "\n",
        "            for i in range(9):\n",
        "                # qostaushy's part\n",
        "                plt.scatter(np.repeat(points_bastaushy_x + 25 * i, 5)[:self.otaular[17 - i]], points_bastaushy_y[:self.otaular[17 - i]], marker='o')\n",
        "                # horizontal line\n",
        "                plt.plot(x, np.repeat(y, len(x)))\n",
        "                # vertical lines\n",
        "                plt.plot(np.repeat(25 * i - 2, len(x)), np.arange(-7, 5, 12 / len(x)))\n",
        "                # bastaushy's part\n",
        "                plt.scatter(np.repeat(points_bastaushy_x + 25 * i, 5)[:self.otaular[i]], points_bastaushy_y[:self.otaular[i]] - 6, marker='o')\n",
        "\n",
        "            #last vertical line\n",
        "            plt.plot(np.repeat(25 * 9 - 2, len(x)), np.arange(-7, 5, 12 / len(x)))\n",
        "\n",
        "            for i in range(9):\n",
        "                # bastaushy's qumalaqtar\n",
        "                plt.text(25 * i + 10, -7, f'{i} ({self.otaular[i]})', **text_kwargs)\n",
        "                # qostaushy's qumalaqtar\n",
        "                plt.text(25 * i + 10, 5, f'{17 - i} ({self.otaular[17 - i]})', **text_kwargs)\n",
        "            # bastaushy qazan's qumalaqtar\n",
        "            plt.text(230, -4, f'qazan: {self.qazandar[0]}', **text_kwargs)\n",
        "            # qostaushy qazan's qumalaqtar\n",
        "            plt.text(230, 2, f'qazan: {self.qazandar[1]}', **text_kwargs);\n",
        "            # bastaushy tuzdyq's qumalaqtar\n",
        "            plt.text(230, -6, f'tuzdyq: {self.tuzdyq[0]}', **text_kwargs)\n",
        "            # qostaushy tuzdyq's qumalaqtar\n",
        "            plt.text(230, 0, f'tuzdyq: {self.tuzdyq[1]}', **text_kwargs);\n",
        "            plt.show()\n",
        "        else:\n",
        "            if self.render_mode == \"human\":\n",
        "                print(\"Game over\")\n",
        "\n",
        "    def _legal_moves(self, agent):\n",
        "        cur_player = self.possible_agents.index(agent)\n",
        "        opp_player = (cur_player + 1) % 2\n",
        "        return [item for item in range(9 * cur_player, (cur_player + 1) * 9) if self.tuzdyq[opp_player] != item and self.otaular[item] > 0]\n",
        "\n",
        "    def observe(self, agent):\n",
        "        \"\"\"\n",
        "        Observe should return the observation of the specified agent. This function\n",
        "        should return a sane observation (though not necessarily the most up to date possible)\n",
        "        at any time after reset() is called.\n",
        "        \"\"\"\n",
        "        # observation of one agent is the previous state of the other\n",
        "        legal_moves = self._legal_moves(agent) if agent == self.agent_selection else []\n",
        "        action_mask = np.zeros(9, \"int8\")\n",
        "        if self.possible_agents.index(agent) == 1:\n",
        "            legal_moves = [i - 9 for i in legal_moves]\n",
        "        for i in legal_moves:\n",
        "            action_mask[i] = 1\n",
        "        observation = tuple(\n",
        "            self.otaular + self.tuzdyq + self.qazandar\n",
        "        )\n",
        "        return {\"observation\": observation, \"action_mask\": action_mask}\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Close should release any graphical displays, subprocesses, network connections\n",
        "        or any other environment data which should not be kept around after the\n",
        "        user is no longer using the environment.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset needs to initialize the following attributes\n",
        "        - agents\n",
        "        - rewards\n",
        "        - _cumulative_rewards\n",
        "        - terminations\n",
        "        - truncations\n",
        "        - infos\n",
        "        - agent_selection\n",
        "        And must set up the environment so that render(), step(), and observe()\n",
        "        can be called without issues.\n",
        "        Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by step() and observe()\n",
        "        \"\"\"\n",
        "        self.agents = self.possible_agents[:]\n",
        "        self.rewards = {agent: 0 for agent in self.agents}\n",
        "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
        "        self.otaular = [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
        "        self.direction = [list(range(18)), [9, 10, 11, 12, 13, 14, 15, 16, 17, 0, 1, 2, 3, 4, 5, 6, 7, 8]]\n",
        "        self.tuzdyq = [-1, -1]\n",
        "        self.qazandar = [0, 0]\n",
        "        self.terminations = {agent: False for agent in self.agents}\n",
        "        self.truncations = {agent: False for agent in self.agents}\n",
        "        self.infos = {agent: {} for agent in self.agents}\n",
        "        self.num_moves = 0\n",
        "        observation = tuple(\n",
        "            self.otaular + self.tuzdyq + self.qazandar\n",
        "        )\n",
        "        self.observations = {agent: observation for agent in self.agents}\n",
        "        \"\"\"\n",
        "        Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
        "        \"\"\"\n",
        "        self._agent_selector = agent_selector(self.agents)\n",
        "        self.agent_selection = self._agent_selector.next()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        step(action) takes in an action for the current agent (specified by\n",
        "        agent_selection) and needs to update\n",
        "        - rewards\n",
        "        - _cumulative_rewards (accumulating the rewards)\n",
        "        - terminations\n",
        "        - truncations\n",
        "        - infos\n",
        "        - agent_selection (to the next agent)\n",
        "        And any internal state used by observe() or render()\n",
        "        \"\"\"\n",
        "        if (\n",
        "            self.terminations[self.agent_selection]\n",
        "            or self.truncations[self.agent_selection]\n",
        "        ):\n",
        "            # handles stepping an agent which is already dead\n",
        "            # accepts a None action for the one agent, and moves the agent_selection to\n",
        "            # the next dead agent,  or if there are no more dead agents, to the next live agent\n",
        "            self._was_dead_step(action)\n",
        "            return\n",
        "\n",
        "        cur_player = self.possible_agents.index(self.agent_selection)\n",
        "        opp_player = (cur_player + 1) % 2\n",
        "        self.num_moves += 1\n",
        "        if self.render_mode == \"human\":\n",
        "            print(f'MOVE #{self.num_moves}')\n",
        "        # The truncations dictionary must be updated for all players.\n",
        "        self.truncations = {\n",
        "            agent: self.num_moves >= NUM_ITERS for agent in self.agents\n",
        "        }\n",
        "        # distribute qumalaqs\n",
        "        if cur_player == 1:\n",
        "            action += 9\n",
        "        if self.render_mode == \"human\":\n",
        "            print(f'{self.agent_selection} made action {action}')\n",
        "        num_qumalaq = self.otaular[action]\n",
        "        idx_action = self.direction[cur_player].index(action)\n",
        "        if self.otaular[action] == 1:\n",
        "            self.otaular[self.direction[cur_player][idx_action + 1]] += 1\n",
        "            self.otaular[action] -= 1\n",
        "        else:\n",
        "            i = 1\n",
        "            while self.otaular[action] > 1:\n",
        "                self.otaular[self.direction[cur_player][(idx_action + i) % 18]] += 1\n",
        "                self.otaular[action] -= 1\n",
        "                i += 1\n",
        "        # check tuzdyq & add rewards to qazandar\n",
        "        reward = 0\n",
        "        if self.check_tuzdyq(self.agent_selection, action):\n",
        "            reward += 3\n",
        "            if self.render_mode == \"human\":\n",
        "                print(f'{self.agent_selection} won tuzdyq {reward}')\n",
        "        else:\n",
        "\n",
        "            if num_qumalaq > 1:\n",
        "                last_otau = self.direction[cur_player][(idx_action + num_qumalaq - 1) % 18]\n",
        "            else:\n",
        "                last_otau = self.direction[cur_player][(idx_action + num_qumalaq) % 18]\n",
        "\n",
        "            if last_otau in range(opp_player * 9, (opp_player + 1) * 9) and self.otaular[last_otau] % 2 == 0:\n",
        "                reward += self.otaular[last_otau]\n",
        "                if self.render_mode == \"human\":\n",
        "                    print(f'{self.agent_selection} won {reward}')\n",
        "                self.otaular[last_otau] = 0\n",
        "            if self.tuzdyq[cur_player] >= 0 and self.otaular[self.tuzdyq[cur_player]] > 0:\n",
        "                reward += self.otaular[self.tuzdyq[cur_player]]\n",
        "                if self.render_mode == \"human\":\n",
        "                    print(f'{self.agent_selection} won tuzdyq {self.otaular[self.tuzdyq[cur_player]]}')\n",
        "                self.otaular[self.tuzdyq[cur_player]] = 0\n",
        "        if self.render_mode == \"human\":\n",
        "            print(f'{self.agent_selection} won total {reward}')\n",
        "        self.qazandar[cur_player] += reward\n",
        "        self.rewards[self.agent_selection] += reward\n",
        "        # check if there is a winner\n",
        "        winner = self.check_for_winner()\n",
        "        if winner:\n",
        "            self.terminations = {i: True for i in self.agents}\n",
        "            if self.render_mode == \"human\":\n",
        "                print(f'{self.agent_selection} won the game!!!')\n",
        "        # selects the next agent.\n",
        "        self.agent_selection = self._agent_selector.next()\n",
        "        # Adds .rewards to ._cumulative_rewards\n",
        "        self._accumulate_rewards()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "    def check_tuzdyq(self, agent, action):\n",
        "        cur_player = self.possible_agents.index(agent)\n",
        "        opp_player = (cur_player + 1) % 2\n",
        "        idx = self.direction[cur_player].index(action)\n",
        "        num_qumalaq = self.otaular[action]\n",
        "\n",
        "        if num_qumalaq > 1:\n",
        "            last_otau = self.direction[cur_player][(idx + num_qumalaq - 1) % 18]\n",
        "        else:\n",
        "            last_otau = self.direction[cur_player][(idx + num_qumalaq) % 18]\n",
        "\n",
        "        if last_otau in range(opp_player * 9, (opp_player + 1) * 9) and self.otaular[last_otau] == 3 and last_otau != 17 - cur_player * 9 and abs(last_otau - self.tuzdyq[opp_player]) != 9:\n",
        "            self.tuzdyq[cur_player] = last_otau\n",
        "            self.otaular[last_otau] = 0\n",
        "            if self.render_mode == \"human\":\n",
        "                print(f'{agent} got tuzdyq {last_otau}!')\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def check_atsyrau(self, agent):\n",
        "        cur_player = self.possible_agents.index(agent)\n",
        "        opp_player = (cur_player + 1) % 2\n",
        "\n",
        "        for idx, i in enumerate(self.otaular[cur_player * 9: (cur_player + 1) * 9]):\n",
        "            if i > 0 and idx + cur_player * 9 != self.tuzdyq[opp_player]:\n",
        "                return False\n",
        "        if self.render_mode == \"human\":\n",
        "            print(f'{agent} reached atsyrau')\n",
        "        return True\n",
        "\n",
        "    def check_for_winner(self):\n",
        "        cur_player = self.possible_agents.index(self.agent_selection)\n",
        "        opp_player = (cur_player + 1) % 2\n",
        "        if self.qazandar[cur_player] > 81:\n",
        "            PLAYS[self.agent_selection] += 1\n",
        "            return True\n",
        "        if self.check_atsyrau(self.possible_agents[opp_player]) and self.qazandar[opp_player] <= 81:\n",
        "            PLAYS[self.agent_selection] += 1\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ¦š Testing environment"
      ],
      "metadata": {
        "id": "46_0KK8E1rWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# env = env(render_mode=\"human\")\n",
        "# env.reset(seed=42)\n",
        "\n",
        "# for agent in env.agent_iter():\n",
        "#     observation, reward, termination, truncation, info = env.last()\n",
        "\n",
        "#     if termination or truncation:\n",
        "#         action = None\n",
        "#     else:\n",
        "#         mask = observation[\"action_mask\"]\n",
        "#         # this is where you would insert your policy\n",
        "#         action = env.action_space(agent).sample(mask)\n",
        "\n",
        "#     env.step(action)\n",
        "# env.close()"
      ],
      "metadata": {
        "id": "lulAELFM0LZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¼ DQN agent to play vs a random policy agent https://pettingzoo.farama.org/tutorials/tianshou/intermediate/"
      ],
      "metadata": {
        "id": "ebqZJ9uJ1qKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ¡ Additional installations https://github.com/thu-ml/tianshou"
      ],
      "metadata": {
        "id": "nF0ZMW3wLv-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging==21.3\n",
        "!pip install tianshou==0.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GBZ4IbRNLr9c",
        "outputId": "f47bcfc6-634b-49e0-ff41-e4d88e27de31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting packaging==21.3\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging==21.3) (3.1.2)\n",
            "Installing collected packages: packaging\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed packaging-21.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "a9d7b5195dad4e80ab2c7a252e244fee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tianshou==0.5.0\n",
            "  Downloading tianshou-0.5.0-py3-none-any.whl (162 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/162.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.0/162.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (0.29.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (4.66.2)\n",
            "Requirement already satisfied: numpy>1.16.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (1.25.2)\n",
            "Requirement already satisfied: tensorboard>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (2.15.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (0.58.1)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (3.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.0) (21.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.26.0->tianshou==0.5.0) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.26.0->tianshou==0.5.0) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.26.0->tianshou==0.5.0) (0.0.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->tianshou==0.5.0) (0.41.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.0) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.0) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->tianshou==0.5.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging->tianshou==0.5.0) (3.1.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.5.0->tianshou==0.5.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.5.0->tianshou==0.5.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.5.0->tianshou==0.5.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.5.0->tianshou==0.5.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.5.0->tianshou==0.5.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.5.0->tianshou==0.5.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->tianshou==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.5.0->tianshou==0.5.0) (3.2.2)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tianshou\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 tianshou-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ Imports"
      ],
      "metadata": {
        "id": "T7IIe3E9M69q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "import torch\n",
        "from copy import deepcopy\n",
        "from tianshou.data import Collector, VectorReplayBuffer\n",
        "from tianshou.env import DummyVectorEnv\n",
        "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
        "from tianshou.policy import BasePolicy, DQNPolicy, RainbowPolicy, MultiAgentPolicyManager, RandomPolicy\n",
        "from tianshou.trainer import offpolicy_trainer\n",
        "from tianshou.utils.net.common import Net"
      ],
      "metadata": {
        "id": "TXt2ni4SM-XL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ« Prepare main functions"
      ],
      "metadata": {
        "id": "sNPVNMBTNW3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent1_path = \"policy_128x256x256x128_bs64.pth\"\n",
        "agent2_path = \"policy_256x512x512x256_bs128.pth\"\n",
        "agent3_path = \"policy_512x1024x1024x512_bs128.pth\""
      ],
      "metadata": {
        "id": "zIOctxs50cID"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_agents(\n",
        "    agent_learn: Optional[BasePolicy] = None,\n",
        "    agent_opponent: Optional[BasePolicy] = None,\n",
        "    optim: Optional[torch.optim.Optimizer] = None,\n",
        ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
        "    env = _get_env()\n",
        "    observation_space = (\n",
        "        env.observation_space[\"observation\"]\n",
        "        if isinstance(env.observation_space, gymnasium.spaces.Dict)\n",
        "        else env.observation_space\n",
        "    )\n",
        "    if agent_learn is None:\n",
        "        # model\n",
        "        net = Net(\n",
        "            state_shape=observation_space.shape or observation_space.n,\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[512, 1024, 1024, 512],\n",
        "            #hidden_sizes=[1024, 2048, 2048, 1024],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        if optim is None:\n",
        "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
        "        agent_learn = DQNPolicy(\n",
        "            model=net,\n",
        "            optim=optim,\n",
        "            discount_factor=0.9,\n",
        "            estimation_step=3,\n",
        "            target_update_freq=320,\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    if agent_opponent is None:\n",
        "        if opponent_path:\n",
        "            agent_opponent = deepcopy(agent_learn)\n",
        "            agent_opponent.load_state_dict(torch.load(opponent_path))\n",
        "        else:\n",
        "            agent_opponent = RandomPolicy(action_space=env.action_space)\n",
        "\n",
        "    agents = [agent_opponent, agent_learn]\n",
        "    #agents = [agent_learn, agent_opponent]\n",
        "    policy = MultiAgentPolicyManager(agents, env)\n",
        "    return policy, optim, env.agents\n",
        "\n",
        "\n",
        "def _get_env():\n",
        "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
        "    return PettingZooEnv(env())"
      ],
      "metadata": {
        "id": "9VNyTKoxNZvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  ðŸ‘ Training code https://tianshou.org/en/stable/01_tutorials/04_tictactoe.html"
      ],
      "metadata": {
        "id": "b9q8lRmx-FU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before evaluate this cell run the cell with env\n",
        "\n",
        "# ======== Step 1: Environment setup =========\n",
        "train_envs = DummyVectorEnv([_get_env for _ in range(100)])\n",
        "test_envs = DummyVectorEnv([_get_env for _ in range(100)])\n",
        "\n",
        "# seed\n",
        "seed = 777\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "train_envs.seed(seed)\n",
        "test_envs.seed(seed)\n",
        "\n",
        "# ======== Step 2: Agent setup =========\n",
        "policy, optim, agents = _get_agents()\n",
        "\n",
        "# # ======== Step 3: Collector setup =========\n",
        "train_collector = Collector(\n",
        "    policy,\n",
        "    train_envs,\n",
        "    VectorReplayBuffer(20_000, len(train_envs)),\n",
        "    exploration_noise=True,\n",
        ")\n",
        "test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
        "# policy.set_eps(1)\n",
        "train_collector.collect(n_step=128 * 100)  # batch size * training_num\n",
        "\n",
        "# ======== Step 4: Callback functions setup =========\n",
        "def save_best_fn(policy):\n",
        "    model_save_path = os.path.join(\"log\", \"ttt\", \"dqn\", \"policy.pth\")\n",
        "    os.makedirs(os.path.join(\"log\", \"ttt\", \"dqn\"), exist_ok=True)\n",
        "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
        "\n",
        "def stop_fn(mean_rewards):\n",
        "    return mean_rewards >= 21000\n",
        "\n",
        "def train_fn(epoch, env_step):\n",
        "    policy.policies[agents[1]].set_eps(0.1)\n",
        "\n",
        "def test_fn(epoch, env_step):\n",
        "    policy.policies[agents[1]].set_eps(0.05)\n",
        "\n",
        "def reward_metric(rews):\n",
        "    return rews[:, 1]\n",
        "\n",
        "# ======== Step 5: Run the trainer =========\n",
        "result = offpolicy_trainer(\n",
        "    policy=policy,\n",
        "    train_collector=train_collector,\n",
        "    test_collector=test_collector,\n",
        "    max_epoch=100,\n",
        "    step_per_epoch=1000,\n",
        "    step_per_collect=50,\n",
        "    episode_per_test=10,\n",
        "    batch_size=128,\n",
        "    train_fn=train_fn,\n",
        "    test_fn=test_fn,\n",
        "    stop_fn=stop_fn,\n",
        "    save_best_fn=save_best_fn,\n",
        "    update_per_step=0.1,\n",
        "    test_in_train=False,\n",
        "    reward_metric=reward_metric,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# return result, policy.policies[agents[1]]\n",
        "print(f\"\\n==========Result==========\\n{result}\")\n",
        "print(\"\\n(the trained policy can be accessed via policy.policies[agents[1]])\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvKxESZA-H4y",
        "outputId": "4e768e3d-141d-478d-96ca-2c5ccbfca738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #1: 1001it [00:12, 80.67it/s, bastaushy/loss=270.057, env_step=1000, len=0, n/ep=0, n/st=100, qostaushy/loss=2765.895, rew=0.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1: test_reward: 10799.400000 Â± 3947.835564, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #2: 1001it [00:10, 97.18it/s, bastaushy/loss=111.903, env_step=2000, len=148, n/ep=1, n/st=100, qostaushy/loss=136.716, rew=5858.00]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #2: test_reward: 12951.400000 Â± 2550.261524, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #3: 1001it [00:10, 99.00it/s, bastaushy/loss=201.262, env_step=3000, len=154, n/ep=0, n/st=100, qostaushy/loss=93.804, rew=6295.00]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #3: test_reward: 12856.100000 Â± 5922.258428, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #4: 1001it [00:09, 101.29it/s, bastaushy/loss=473.575, env_step=4000, len=166, n/ep=0, n/st=100, qostaushy/loss=524.497, rew=7274.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #4: test_reward: 13995.600000 Â± 4737.990950, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #5: 1001it [00:09, 100.59it/s, bastaushy/loss=598.521, env_step=5000, len=170, n/ep=0, n/st=100, qostaushy/loss=222.464, rew=6634.60]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #5: test_reward: 14260.900000 Â± 2920.463369, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #6: 1001it [00:10, 98.88it/s, bastaushy/loss=816.723, env_step=6000, len=180, n/ep=0, n/st=100, qostaushy/loss=275.041, rew=7041.00]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #6: test_reward: 11720.700000 Â± 4819.362324, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #7: 1001it [00:10, 93.40it/s, bastaushy/loss=883.190, env_step=7000, len=180, n/ep=0, n/st=100, qostaushy/loss=585.974, rew=7041.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #7: test_reward: 9399.500000 Â± 2530.931261, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #8: 1001it [00:11, 84.99it/s, bastaushy/loss=684.352, env_step=8000, len=180, n/ep=0, n/st=100, qostaushy/loss=536.661, rew=7041.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #8: test_reward: 8197.900000 Â± 4291.245611, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #9: 1001it [00:12, 79.36it/s, bastaushy/loss=735.293, env_step=9000, len=180, n/ep=0, n/st=100, qostaushy/loss=653.424, rew=7041.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #9: test_reward: 15532.700000 Â± 4497.339237, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #10: 1001it [00:11, 83.66it/s, bastaushy/loss=763.006, env_step=10000, len=180, n/ep=0, n/st=100, qostaushy/loss=936.676, rew=7041.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #10: test_reward: 10097.200000 Â± 3728.826137, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #11: 1001it [00:12, 83.29it/s, bastaushy/loss=455.495, env_step=11000, len=180, n/ep=0, n/st=100, qostaushy/loss=978.914, rew=7041.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #11: test_reward: 12497.200000 Â± 4128.103991, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #12: 1001it [00:12, 77.26it/s, bastaushy/loss=626.474, env_step=12000, len=244, n/ep=0, n/st=100, qostaushy/loss=767.202, rew=11362.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #12: test_reward: 9101.500000 Â± 3260.406179, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #13: 1001it [00:12, 80.87it/s, bastaushy/loss=639.062, env_step=13000, len=257, n/ep=0, n/st=100, qostaushy/loss=1180.969, rew=11872.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #13: test_reward: 10155.800000 Â± 3471.003826, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #14: 1001it [00:12, 80.01it/s, bastaushy/loss=981.792, env_step=14000, len=268, n/ep=1, n/st=100, qostaushy/loss=1955.858, rew=12994.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #14: test_reward: 10373.600000 Â± 4874.737207, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #15: 1001it [00:12, 78.94it/s, bastaushy/loss=989.711, env_step=15000, len=278, n/ep=2, n/st=100, qostaushy/loss=2034.027, rew=13697.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #15: test_reward: 9822.800000 Â± 4432.049928, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #16: 1001it [00:12, 77.69it/s, bastaushy/loss=1408.390, env_step=16000, len=108, n/ep=1, n/st=100, qostaushy/loss=1664.387, rew=4890.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #16: test_reward: 3575.400000 Â± 2611.969992, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #17: 1001it [00:12, 79.23it/s, bastaushy/loss=1312.736, env_step=17000, len=150, n/ep=1, n/st=100, qostaushy/loss=2415.719, rew=9360.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #17: test_reward: 14613.800000 Â± 6818.803734, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #18: 1001it [00:12, 79.23it/s, bastaushy/loss=1431.187, env_step=18000, len=120, n/ep=0, n/st=100, qostaushy/loss=2831.133, rew=6282.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #18: test_reward: 13140.400000 Â± 2904.572092, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #19: 1001it [00:12, 78.67it/s, bastaushy/loss=1409.744, env_step=19000, len=229, n/ep=0, n/st=100, qostaushy/loss=2360.154, rew=12944.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #19: test_reward: 7379.500000 Â± 4258.406844, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #20: 1001it [00:12, 78.66it/s, bastaushy/loss=1657.724, env_step=20000, len=328, n/ep=1, n/st=100, qostaushy/loss=3422.645, rew=17544.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #20: test_reward: 12545.400000 Â± 6740.766903, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #21: 1001it [00:13, 74.23it/s, bastaushy/loss=1801.802, env_step=21000, len=337, n/ep=0, n/st=100, qostaushy/loss=3687.742, rew=16784.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #21: test_reward: 10545.500000 Â± 3541.176930, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #22: 1001it [00:12, 78.12it/s, bastaushy/loss=2433.778, env_step=22000, len=348, n/ep=1, n/st=100, qostaushy/loss=3883.549, rew=18844.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #22: test_reward: 9799.400000 Â± 2893.567494, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #23: 1001it [00:13, 76.90it/s, bastaushy/loss=2677.015, env_step=23000, len=358, n/ep=1, n/st=100, qostaushy/loss=4529.737, rew=19624.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #23: test_reward: 10819.200000 Â± 4794.280004, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #24: 1001it [00:12, 77.67it/s, bastaushy/loss=2335.423, env_step=24000, len=277, n/ep=0, n/st=100, qostaushy/loss=4580.123, rew=15558.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #24: test_reward: 10581.800000 Â± 4239.096809, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #25: 1001it [00:12, 77.46it/s, bastaushy/loss=2485.229, env_step=25000, len=197, n/ep=0, n/st=100, qostaushy/loss=5355.015, rew=10176.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #25: test_reward: 11015.400000 Â± 4182.801267, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #26: 1001it [00:12, 77.39it/s, bastaushy/loss=2781.117, env_step=26000, len=387, n/ep=0, n/st=100, qostaushy/loss=4772.597, rew=22152.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #26: test_reward: 10606.200000 Â± 3169.226335, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #27: 1001it [00:12, 77.21it/s, bastaushy/loss=2691.468, env_step=27000, len=397, n/ep=0, n/st=100, qostaushy/loss=4088.952, rew=22572.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #27: test_reward: 11041.300000 Â± 4970.573771, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #28: 1001it [00:12, 77.64it/s, bastaushy/loss=2705.067, env_step=28000, len=147, n/ep=0, n/st=100, qostaushy/loss=5230.582, rew=8956.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #28: test_reward: 13388.200000 Â± 3260.883156, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #29: 1001it [00:13, 76.32it/s, bastaushy/loss=2741.746, env_step=29000, len=129, n/ep=0, n/st=100, qostaushy/loss=4978.288, rew=5016.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #29: test_reward: 10221.600000 Â± 4511.669296, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #30: 1001it [00:13, 76.76it/s, bastaushy/loss=2831.437, env_step=30000, len=159, n/ep=1, n/st=100, qostaushy/loss=4258.850, rew=7200.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #30: test_reward: 13038.400000 Â± 3691.890984, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #31: 1001it [00:13, 72.87it/s, bastaushy/loss=2694.364, env_step=31000, len=180, n/ep=0, n/st=100, qostaushy/loss=5100.143, rew=11610.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #31: test_reward: 16332.500000 Â± 4730.730879, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #32: 1001it [00:13, 76.04it/s, bastaushy/loss=2730.338, env_step=32000, len=268, n/ep=1, n/st=100, qostaushy/loss=5302.998, rew=15984.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #32: test_reward: 13007.800000 Â± 5306.173325, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #33: 1001it [00:13, 75.43it/s, bastaushy/loss=2982.550, env_step=33000, len=107, n/ep=0, n/st=100, qostaushy/loss=4736.273, rew=6389.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #33: test_reward: 10599.900000 Â± 4619.962261, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #34: 1001it [00:13, 76.74it/s, bastaushy/loss=3236.827, env_step=34000, len=216, n/ep=0, n/st=100, qostaushy/loss=4348.962, rew=13831.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #34: test_reward: 10623.000000 Â± 4280.000327, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #35: 1001it [00:13, 76.73it/s, bastaushy/loss=2804.256, env_step=35000, len=307, n/ep=0, n/st=100, qostaushy/loss=5424.095, rew=17352.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #35: test_reward: 12310.900000 Â± 3191.417097, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #36: 1001it [00:13, 76.40it/s, bastaushy/loss=3199.747, env_step=36000, len=160, n/ep=1, n/st=100, qostaushy/loss=4579.332, rew=9366.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #36: test_reward: 7280.900000 Â± 2736.021983, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #37: 1001it [00:13, 76.52it/s, bastaushy/loss=2862.584, env_step=37000, len=237, n/ep=2, n/st=100, qostaushy/loss=4713.579, rew=15526.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #37: test_reward: 15030.800000 Â± 3705.796130, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #38: 1001it [00:13, 76.39it/s, bastaushy/loss=2898.049, env_step=38000, len=223, n/ep=1, n/st=100, qostaushy/loss=4181.373, rew=12612.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #38: test_reward: 8566.100000 Â± 3622.231093, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #39: 1001it [00:13, 74.83it/s, bastaushy/loss=2612.066, env_step=39000, len=132, n/ep=1, n/st=100, qostaushy/loss=4791.786, rew=7474.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #39: test_reward: 11453.600000 Â± 4060.008355, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #40: 1001it [00:13, 72.77it/s, bastaushy/loss=3368.396, env_step=40000, len=220, n/ep=2, n/st=100, qostaushy/loss=4234.125, rew=10512.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #40: test_reward: 12798.700000 Â± 5812.412048, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #41: 1001it [00:13, 76.76it/s, bastaushy/loss=3264.767, env_step=41000, len=203, n/ep=0, n/st=100, qostaushy/loss=4032.743, rew=12336.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #41: test_reward: 13885.700000 Â± 2871.938789, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #42: 1001it [00:13, 76.44it/s, bastaushy/loss=2670.233, env_step=42000, len=210, n/ep=0, n/st=100, qostaushy/loss=3927.700, rew=10710.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #42: test_reward: 12555.400000 Â± 4405.251325, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #43: 1001it [00:13, 76.69it/s, bastaushy/loss=2716.544, env_step=43000, len=147, n/ep=0, n/st=100, qostaushy/loss=3284.738, rew=6155.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #43: test_reward: 16349.800000 Â± 5013.833779, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #44: 1001it [00:13, 75.43it/s, bastaushy/loss=3080.175, env_step=44000, len=235, n/ep=0, n/st=100, qostaushy/loss=2940.202, rew=14148.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #44: test_reward: 10581.200000 Â± 1418.388720, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #45: 1001it [00:13, 76.59it/s, bastaushy/loss=3130.203, env_step=45000, len=195, n/ep=1, n/st=100, qostaushy/loss=3403.299, rew=7378.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #45: test_reward: 11046.500000 Â± 4401.005732, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #46: 1001it [00:13, 76.65it/s, bastaushy/loss=3104.728, env_step=46000, len=257, n/ep=1, n/st=100, qostaushy/loss=3427.163, rew=10972.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #46: test_reward: 12548.800000 Â± 4233.680923, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #47: 1001it [00:13, 76.44it/s, bastaushy/loss=3109.147, env_step=47000, len=234, n/ep=0, n/st=100, qostaushy/loss=2926.756, rew=9484.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #47: test_reward: 15653.100000 Â± 5465.432214, best_reward: 16625.700000 Â± 4250.630260 in #0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #48: 1001it [00:13, 76.11it/s, bastaushy/loss=2881.930, env_step=48000, len=98, n/ep=0, n/st=100, qostaushy/loss=3215.308, rew=5240.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #48: test_reward: 16744.600000 Â± 4604.044683, best_reward: 16744.600000 Â± 4604.044683 in #48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #49: 1001it [00:14, 69.02it/s, bastaushy/loss=3260.083, env_step=49000, len=224, n/ep=1, n/st=100, qostaushy/loss=3693.429, rew=14386.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #49: test_reward: 14534.600000 Â± 6140.554685, best_reward: 16744.600000 Â± 4604.044683 in #48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #50: 1001it [00:13, 76.08it/s, bastaushy/loss=3244.766, env_step=50000, len=288, n/ep=0, n/st=100, qostaushy/loss=3128.700, rew=18776.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #50: test_reward: 17288.600000 Â± 5902.201813, best_reward: 17288.600000 Â± 5902.201813 in #50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #51: 1001it [00:13, 76.01it/s, bastaushy/loss=3081.336, env_step=51000, len=212, n/ep=0, n/st=100, qostaushy/loss=3460.241, rew=8507.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #51: test_reward: 14026.700000 Â± 6472.901761, best_reward: 17288.600000 Â± 5902.201813 in #50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #52: 1001it [00:13, 76.06it/s, bastaushy/loss=3416.710, env_step=52000, len=161, n/ep=0, n/st=100, qostaushy/loss=3494.780, rew=7452.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #52: test_reward: 20748.800000 Â± 2072.252195, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #53: 1001it [00:13, 75.87it/s, bastaushy/loss=2654.459, env_step=53000, len=190, n/ep=0, n/st=100, qostaushy/loss=2582.663, rew=10834.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #53: test_reward: 15457.900000 Â± 5498.025290, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #54: 1001it [00:13, 74.73it/s, bastaushy/loss=3385.670, env_step=54000, len=280, n/ep=0, n/st=100, qostaushy/loss=2538.109, rew=16726.50]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #54: test_reward: 13023.300000 Â± 5589.134997, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #55: 1001it [00:13, 75.85it/s, bastaushy/loss=3291.726, env_step=55000, len=287, n/ep=0, n/st=100, qostaushy/loss=2410.184, rew=16756.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #55: test_reward: 12000.200000 Â± 7099.538208, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #56: 1001it [00:13, 75.56it/s, bastaushy/loss=2927.545, env_step=56000, len=310, n/ep=0, n/st=100, qostaushy/loss=2941.991, rew=21228.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #56: test_reward: 15300.300000 Â± 6729.148075, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #57: 1001it [00:13, 75.84it/s, bastaushy/loss=2678.108, env_step=57000, len=244, n/ep=1, n/st=100, qostaushy/loss=2473.068, rew=16056.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #57: test_reward: 15055.100000 Â± 6612.225532, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #58: 1001it [00:14, 70.57it/s, bastaushy/loss=3294.424, env_step=58000, len=160, n/ep=0, n/st=100, qostaushy/loss=3074.328, rew=10499.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #58: test_reward: 13352.400000 Â± 6877.931334, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #59: 1001it [00:13, 75.86it/s, bastaushy/loss=2299.836, env_step=59000, len=173, n/ep=0, n/st=100, qostaushy/loss=3001.926, rew=9232.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #59: test_reward: 15514.700000 Â± 4336.569429, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #60: 1001it [00:13, 75.56it/s, bastaushy/loss=2742.782, env_step=60000, len=217, n/ep=0, n/st=100, qostaushy/loss=2992.686, rew=9420.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #60: test_reward: 8210.100000 Â± 3280.578438, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #61: 1001it [00:13, 75.58it/s, bastaushy/loss=2619.596, env_step=61000, len=284, n/ep=0, n/st=100, qostaushy/loss=3016.771, rew=17933.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #61: test_reward: 9066.700000 Â± 2982.469247, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #62: 1001it [00:13, 75.57it/s, bastaushy/loss=2206.060, env_step=62000, len=400, n/ep=1, n/st=100, qostaushy/loss=2397.644, rew=24226.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #62: test_reward: 13329.800000 Â± 4608.334662, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #63: 1001it [00:13, 74.54it/s, bastaushy/loss=2668.290, env_step=63000, len=181, n/ep=0, n/st=100, qostaushy/loss=2534.332, rew=8492.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #63: test_reward: 18716.500000 Â± 6916.441300, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #64: 1001it [00:13, 75.34it/s, bastaushy/loss=2326.234, env_step=64000, len=156, n/ep=0, n/st=100, qostaushy/loss=2700.535, rew=8857.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #64: test_reward: 15823.300000 Â± 4716.217044, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #65: 1001it [00:13, 75.53it/s, bastaushy/loss=2526.801, env_step=65000, len=240, n/ep=0, n/st=100, qostaushy/loss=3368.495, rew=14795.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #65: test_reward: 17916.300000 Â± 4432.371488, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #66: 1001it [00:13, 75.16it/s, bastaushy/loss=2131.056, env_step=66000, len=244, n/ep=0, n/st=100, qostaushy/loss=2976.709, rew=16516.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #66: test_reward: 11499.300000 Â± 4424.930328, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #67: 1001it [00:13, 71.51it/s, bastaushy/loss=1837.717, env_step=67000, len=400, n/ep=0, n/st=100, qostaushy/loss=3022.221, rew=23300.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #67: test_reward: 9558.400000 Â± 4701.440209, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #68: 1001it [00:13, 72.52it/s, bastaushy/loss=2277.522, env_step=68000, len=314, n/ep=1, n/st=100, qostaushy/loss=2997.935, rew=19893.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #68: test_reward: 13447.000000 Â± 5990.952362, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #69: 1001it [00:13, 74.82it/s, bastaushy/loss=2235.950, env_step=69000, len=234, n/ep=0, n/st=100, qostaushy/loss=3193.916, rew=13405.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #69: test_reward: 9714.600000 Â± 3464.039036, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #70: 1001it [00:13, 74.44it/s, bastaushy/loss=1547.948, env_step=70000, len=297, n/ep=0, n/st=100, qostaushy/loss=3117.144, rew=18500.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #70: test_reward: 14458.900000 Â± 4044.083319, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #71: 1001it [00:13, 74.44it/s, bastaushy/loss=2067.592, env_step=71000, len=116, n/ep=1, n/st=100, qostaushy/loss=2928.147, rew=5634.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #71: test_reward: 10292.900000 Â± 2987.292368, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #72: 1001it [00:13, 74.68it/s, bastaushy/loss=2181.316, env_step=72000, len=228, n/ep=0, n/st=100, qostaushy/loss=2995.118, rew=14959.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #72: test_reward: 12555.900000 Â± 3485.855259, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #73: 1001it [00:13, 72.52it/s, bastaushy/loss=2009.769, env_step=73000, len=106, n/ep=0, n/st=100, qostaushy/loss=2860.566, rew=6128.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #73: test_reward: 10363.700000 Â± 3886.847721, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #74: 1001it [00:13, 74.79it/s, bastaushy/loss=1848.589, env_step=74000, len=236, n/ep=0, n/st=100, qostaushy/loss=3166.375, rew=16244.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #74: test_reward: 13097.900000 Â± 4184.898026, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #75: 1001it [00:13, 74.97it/s, bastaushy/loss=1864.220, env_step=75000, len=257, n/ep=0, n/st=100, qostaushy/loss=3361.660, rew=11212.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #75: test_reward: 12967.400000 Â± 3978.599256, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #76: 1001it [00:13, 73.92it/s, bastaushy/loss=2048.368, env_step=76000, len=234, n/ep=1, n/st=100, qostaushy/loss=3728.016, rew=14795.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #76: test_reward: 12237.600000 Â± 4081.394521, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #77: 1001it [00:13, 75.18it/s, bastaushy/loss=2205.551, env_step=77000, len=176, n/ep=2, n/st=100, qostaushy/loss=3835.769, rew=11112.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #77: test_reward: 9331.500000 Â± 4409.580598, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #78: 1001it [00:13, 75.03it/s, bastaushy/loss=2559.547, env_step=78000, len=177, n/ep=1, n/st=100, qostaushy/loss=2743.137, rew=9314.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #78: test_reward: 10208.900000 Â± 4280.859130, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #79: 1001it [00:13, 74.85it/s, bastaushy/loss=2329.673, env_step=79000, len=289, n/ep=2, n/st=100, qostaushy/loss=3216.085, rew=18482.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #79: test_reward: 12727.300000 Â± 5827.241424, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #80: 1001it [00:13, 74.94it/s, bastaushy/loss=2824.043, env_step=80000, len=161, n/ep=0, n/st=100, qostaushy/loss=3655.198, rew=10156.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #80: test_reward: 12727.900000 Â± 4871.339045, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #81: 1001it [00:13, 75.36it/s, bastaushy/loss=2365.626, env_step=81000, len=177, n/ep=2, n/st=100, qostaushy/loss=3326.921, rew=11059.50]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #81: test_reward: 11605.400000 Â± 3269.345353, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #82: 1001it [00:13, 74.42it/s, bastaushy/loss=2473.191, env_step=82000, len=79, n/ep=1, n/st=100, qostaushy/loss=3219.113, rew=3384.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #82: test_reward: 10904.000000 Â± 4138.195090, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #83: 1001it [00:13, 75.16it/s, bastaushy/loss=2525.048, env_step=83000, len=162, n/ep=2, n/st=100, qostaushy/loss=4004.970, rew=9848.50]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #83: test_reward: 12310.800000 Â± 4405.590194, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #84: 1001it [00:13, 75.49it/s, bastaushy/loss=2582.320, env_step=84000, len=400, n/ep=0, n/st=100, qostaushy/loss=3805.256, rew=24794.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #84: test_reward: 10749.500000 Â± 2703.268031, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #85: 1001it [00:13, 75.24it/s, bastaushy/loss=2185.154, env_step=85000, len=242, n/ep=0, n/st=100, qostaushy/loss=3976.371, rew=15780.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #85: test_reward: 18472.900000 Â± 5660.205305, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #86: 1001it [00:13, 71.57it/s, bastaushy/loss=2422.533, env_step=86000, len=135, n/ep=0, n/st=100, qostaushy/loss=3920.253, rew=4648.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #86: test_reward: 13497.400000 Â± 7193.553742, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #87: 1001it [00:13, 73.42it/s, bastaushy/loss=2682.225, env_step=87000, len=152, n/ep=1, n/st=100, qostaushy/loss=4438.722, rew=9912.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #87: test_reward: 7158.000000 Â± 4508.053682, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #88: 1001it [00:13, 74.99it/s, bastaushy/loss=2237.104, env_step=88000, len=400, n/ep=0, n/st=100, qostaushy/loss=3980.785, rew=24408.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #88: test_reward: 8038.300000 Â± 4016.087625, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #89: 1001it [00:13, 75.12it/s, bastaushy/loss=2915.517, env_step=89000, len=400, n/ep=0, n/st=100, qostaushy/loss=3564.886, rew=24408.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #89: test_reward: 9757.100000 Â± 3851.735205, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #90: 1001it [00:13, 74.96it/s, bastaushy/loss=2540.450, env_step=90000, len=103, n/ep=0, n/st=100, qostaushy/loss=3105.911, rew=4652.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #90: test_reward: 7980.400000 Â± 4946.048427, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #91: 1001it [00:13, 74.87it/s, bastaushy/loss=2984.481, env_step=91000, len=138, n/ep=2, n/st=100, qostaushy/loss=3274.651, rew=8212.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #91: test_reward: 9104.000000 Â± 4583.815616, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #92: 1001it [00:13, 74.09it/s, bastaushy/loss=2664.356, env_step=92000, len=122, n/ep=1, n/st=100, qostaushy/loss=3743.359, rew=4946.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #92: test_reward: 15161.500000 Â± 2857.960540, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #93: 1001it [00:13, 74.63it/s, bastaushy/loss=3035.575, env_step=93000, len=148, n/ep=0, n/st=100, qostaushy/loss=3248.003, rew=6562.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #93: test_reward: 17294.200000 Â± 7523.604213, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #94: 1001it [00:13, 74.53it/s, bastaushy/loss=2837.739, env_step=94000, len=178, n/ep=0, n/st=100, qostaushy/loss=3247.127, rew=7611.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #94: test_reward: 12869.200000 Â± 5125.895176, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #95: 1001it [00:14, 70.16it/s, bastaushy/loss=2687.083, env_step=95000, len=400, n/ep=0, n/st=100, qostaushy/loss=3720.616, rew=21426.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #95: test_reward: 15717.000000 Â± 6917.130388, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #96: 1001it [00:13, 74.51it/s, bastaushy/loss=2599.334, env_step=96000, len=273, n/ep=0, n/st=100, qostaushy/loss=3517.812, rew=14336.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #96: test_reward: 12647.300000 Â± 4524.538453, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #97: 1001it [00:13, 71.76it/s, bastaushy/loss=2866.826, env_step=97000, len=115, n/ep=1, n/st=100, qostaushy/loss=3600.567, rew=5494.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #97: test_reward: 15955.500000 Â± 5917.978223, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #98: 1001it [00:13, 74.14it/s, bastaushy/loss=3055.185, env_step=98000, len=142, n/ep=0, n/st=100, qostaushy/loss=3253.939, rew=9411.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #98: test_reward: 11798.900000 Â± 3944.010204, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #99: 1001it [00:13, 74.39it/s, bastaushy/loss=2642.827, env_step=99000, len=201, n/ep=0, n/st=100, qostaushy/loss=3470.781, rew=9676.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #99: test_reward: 8838.900000 Â± 5139.054202, best_reward: 20748.800000 Â± 2072.252195 in #52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch #100: 1001it [00:13, 73.71it/s, bastaushy/loss=2455.270, env_step=100000, len=143, n/ep=0, n/st=100, qostaushy/loss=3359.213, rew=3124.00]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #100: test_reward: 16604.800000 Â± 6628.959749, best_reward: 20748.800000 Â± 2072.252195 in #52\n",
            "\n",
            "==========Result==========\n",
            "{'duration': '1446.99s', 'train_time/model': '1279.12s', 'test_step': 259246, 'test_episode': 1010, 'test_time': '140.15s', 'test_speed': '1849.78 step/s', 'best_reward': 20748.8, 'best_result': '20748.80 Â± 2072.25', 'train_step': 100000, 'train_episode': 474, 'train_time/collector': '27.72s', 'train_speed': '76.52 step/s'}\n",
            "\n",
            "(the trained policy can be accessed via policy.policies[agents[1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ™ Evaluate best Qostaushy agent"
      ],
      "metadata": {
        "id": "ARZK7tZNb4J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PLAYS = {\"bastaushy\": 0, \"qostaushy\": 0}\n",
        "# Step 1: Load the PettingZoo environment\n",
        "env = env()#render_mode=\"human\")\n",
        "\n",
        "# Step 2: Wrap the environment for Tianshou interfacing\n",
        "env = PettingZooEnv(env)\n",
        "\n",
        "# # Step 3: Define policies for each agent\n",
        "policies = MultiAgentPolicyManager([RandomPolicy(), policy.policies[agents[1]]], env)\n",
        "# # Step 4: Convert the env to vector format\n",
        "env = DummyVectorEnv([lambda: env])\n",
        "\n",
        "# # Step 5: Construct the Collector, which interfaces the policies with the vectorised environment\n",
        "collector = Collector(policies, env)\n",
        "\n",
        "# # Step 6: Execute the environment with the agents playing for 1 episode, and render a frame every ? seconds\n",
        "result = collector.collect(n_episode=1000)\n",
        "print(result)\n",
        "print(PLAYS)"
      ],
      "metadata": {
        "id": "-RbkOmmh-04R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a8cfd4-7738-4d02-ded5-9b5ef12a10fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n/ep': 1000, 'n/st': 157498, 'rews': array([[12720.,  9676.],\n",
            "       [ 2384.,   504.],\n",
            "       [ 6010.,  3100.],\n",
            "       ...,\n",
            "       [ 2312.,   668.],\n",
            "       [ 9296.,  3620.],\n",
            "       [10338.,  6500.]]), 'lens': array([207,  43, 127, 281, 179,  69,  57, 143,  79,  55, 214, 147, 233,\n",
            "       356, 101,  63,  55, 400, 386,  75,  53, 400, 314,  41, 215, 163,\n",
            "       141, 121, 165, 227, 183, 161,  57,  63,  99, 143, 313,  81, 118,\n",
            "       121, 161, 239, 161, 348,  89, 155, 125, 125,  57,  61, 159,  45,\n",
            "        45, 169, 161, 169,  83, 239, 302, 326, 111, 183, 400,  93, 163,\n",
            "       119, 167, 292, 191,  89,  55,  97, 141, 149, 372, 163,  79, 143,\n",
            "       125, 137, 115, 336, 222, 125, 183, 111, 171, 117,  69, 331, 165,\n",
            "       269,  95,  47, 113, 147, 197, 119, 183,  91,  83,  81, 103, 143,\n",
            "       151,  95,  37, 269, 308, 187, 119, 107, 234,  87, 197,  65, 109,\n",
            "        61,  59, 147, 263, 141,  57, 306, 259, 272, 185, 117,  93, 392,\n",
            "        73,  55, 107,  91, 352,  73, 113, 101, 155,  75,  67,  49,  85,\n",
            "        83, 111, 125, 125, 149, 141, 259,  87, 121,  61,  43, 219,  53,\n",
            "       155, 283,  67, 141,  85, 400,  95,  83,  99, 113,  61, 173, 400,\n",
            "       197, 326, 119,  81, 400,  53, 163, 242,  85, 103, 296, 201, 103,\n",
            "       145, 378, 141, 115,  73,  81, 105,  77,  87,  77, 173,  51, 230,\n",
            "       189, 131, 149, 143, 111, 205, 169, 384, 181, 131, 380,  91, 213,\n",
            "       113, 292,  87,  63,  91, 145,  63, 159, 107,  97, 165, 131,  59,\n",
            "        59,  71,  79, 123, 362, 189, 237,  99,  41, 141, 275, 145, 131,\n",
            "       135, 179, 197,  91, 111, 322,  41, 161, 180, 312, 366,  71, 113,\n",
            "       103, 155,  85, 161, 215, 299, 151,  41,  75, 129, 127, 357, 177,\n",
            "       161, 115,  65, 197, 273, 400, 155, 207, 400,  47, 115, 151, 322,\n",
            "        75, 247, 149,  95,  85, 129, 123, 161,  99, 314, 105, 173, 117,\n",
            "       325, 119, 105, 167, 159,  83,  99, 307, 103, 334,  79,  61, 282,\n",
            "       109, 215,  65,  63, 173,  75, 119,  83,  93, 182, 173,  61, 147,\n",
            "       103, 141, 149, 177, 400, 197, 175,  81, 127,  73, 345, 151, 131,\n",
            "       221, 119, 229, 149, 243,  91, 137, 115, 147, 284, 109, 400, 297,\n",
            "       219,  67, 231, 169, 113, 137, 125, 307,  55, 109,  71, 189, 105,\n",
            "        57, 275,  67, 159, 400, 173, 205, 221, 135, 123, 239, 328,  93,\n",
            "       171, 235, 139, 199, 139, 299, 129, 332,  67, 173,  75,  99, 121,\n",
            "       119, 249, 147, 105, 173, 384, 109, 215, 400, 113, 117, 242,  97,\n",
            "       187, 227, 157, 193, 330, 205, 105,  39,  73, 268, 169, 239,  53,\n",
            "       181, 107, 171, 127,  61, 207, 189,  77,  93, 103,  81, 151, 119,\n",
            "       115,  85, 135,  75, 169, 127, 212, 121, 294, 310, 155,  63, 103,\n",
            "       135, 270, 147,  65, 105, 111, 354, 313,  75,  75,  59, 105, 127,\n",
            "       119, 139,  51, 183, 177,  91, 121, 109,  65, 400, 123, 137, 372,\n",
            "        55, 144, 165, 167,  95, 157,  55, 161,  69, 133, 131,  69, 400,\n",
            "        97,  97,  99, 115, 127,  81,  33, 190,  85, 149, 161, 109,  91,\n",
            "       333,  83,  93,  65, 253, 123,  95,  99, 242,  95, 113, 175, 125,\n",
            "       149, 113, 293,  59, 105,  83,  97, 113, 266, 400,  79,  69, 135,\n",
            "       165,  99, 107, 205, 139, 179,  49, 348, 105, 298,  77,  65, 365,\n",
            "       322,  93, 113, 223,  57, 295, 195, 117,  55,  81,  53, 207, 380,\n",
            "       135, 149, 159, 129,  67, 159,  65, 259, 149, 400, 115, 400, 111,\n",
            "       303, 121,  97, 183, 125,  59, 101, 193, 196, 127, 297, 191, 117,\n",
            "       133, 264, 151, 131, 113, 123, 119,  69, 231, 191,  57, 167, 141,\n",
            "       137, 121,  93, 111, 127,  59,  45, 131, 133, 321,  37, 400, 211,\n",
            "       145, 251, 153, 119, 269,  81,  65, 105,  75, 316, 151, 167, 252,\n",
            "       105,  93, 199,  91, 117, 135, 286, 105,  71, 366,  93,  71, 217,\n",
            "       352, 225, 338,  93, 129, 135, 400,  87, 115,  69, 201,  85, 259,\n",
            "        83, 201,  99, 229,  97, 175, 153, 294,  97, 103,  51, 107, 273,\n",
            "        55, 141,  89, 394, 141, 272, 101, 183, 111, 177, 262, 101, 171,\n",
            "        77,  73, 101,  71, 400,  75,  61,  95, 195, 274, 101, 149,  77,\n",
            "       179,  81, 249, 157, 135, 257, 400, 173, 272,  77, 298, 101, 145,\n",
            "        69, 123, 161,  69, 121, 165,  49, 125, 119, 179, 167, 129, 137,\n",
            "        93, 129,  89,  79,  61, 169, 400, 227,  55,  73, 103, 127, 301,\n",
            "       175, 155, 209, 400, 101, 165, 103, 123, 151, 101, 221,  53,  53,\n",
            "        97,  75,  99, 183,  91, 305,  79, 133, 107, 167,  87, 255, 241,\n",
            "       109, 149,  43,  33,  87, 105,  63, 141, 247, 103,  75, 317,  89,\n",
            "       145, 378, 400, 131, 179, 273,  73,  67, 354, 139, 141, 129, 197,\n",
            "       181, 141, 294,  85,  95, 161, 111,  69, 347,  45, 235, 119, 163,\n",
            "        71,  75, 269, 205, 283, 251, 131,  69, 143, 183, 101, 157, 400,\n",
            "       165, 151,  71, 279, 119, 201, 219, 197, 143, 153, 133,  57,  83,\n",
            "        89, 250,  89, 205, 322, 400,  83, 163,  83, 143,  43, 173, 191,\n",
            "        53,  85, 267, 165, 199,  89,  93,  85, 133, 107, 241, 115, 141,\n",
            "        79, 101, 111, 151, 109, 287, 249,  53, 151, 153, 109, 105,  91,\n",
            "        63, 197,  61, 326, 233,  87, 273, 249, 141,  83, 163, 135, 143,\n",
            "        99, 219,  83, 141,  89, 157,  75,  79, 113, 269,  79, 181, 153,\n",
            "        99,  93, 157, 169, 194,  63, 369, 145, 149, 202, 305,  91,  93,\n",
            "        95, 400, 144,  79,  65, 103, 312, 400, 103,  83, 189, 313,  73,\n",
            "       121, 344, 117, 129, 107, 145, 173, 183,  91, 236,  59,  51, 201,\n",
            "       129, 217, 109,  57, 139, 167, 284, 304, 157, 228, 183, 193, 137,\n",
            "       376, 151,  53, 315, 101, 213, 113, 394, 306,  97, 203, 366,  53,\n",
            "       152, 166, 177, 101, 185, 179,  89, 137, 192, 143,  89, 109, 163,\n",
            "       197,  59,  95, 101, 261,  37,  97, 121, 115, 131, 122, 119, 147,\n",
            "       111, 195, 127, 131, 101,  55, 211, 153,  89,  61, 127,  51, 113,\n",
            "       135,  71, 245,  71,  75,  91, 127, 400, 101, 237, 145, 139, 151,\n",
            "        37,  47, 348, 101,  77, 187, 129, 275, 307, 109, 377, 117, 400,\n",
            "        85, 215,  85,  91,  91, 190, 277, 147, 180,  49, 151, 167]), 'idxs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'rew': 7209.241, 'len': 157.498, 'rew_std': 5777.70749509864, 'len_std': 89.50278205731931}\n",
            "{'bastaushy': 872, 'qostaushy': 98}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ³ Experiments"
      ],
      "metadata": {
        "id": "pIAdRMbp2zS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "\n",
        " net = Net(\n",
        "            state_shape=observation_space.shape or observation_space.n,\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[128, 256, 256, 128],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_collector.collect(n_step=64 * 100)  # batch size * training_num\n",
        "\n",
        "res: {'bastaushy': 781, 'qostaushy': 194}\n",
        "\n",
        "res: {'bastaushy': 790, 'qostaushy': 191}\n",
        "\n",
        "2.\n",
        "\n",
        " net = Net(\n",
        "            state_shape=observation_space.shape or observation_space.n,\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[128, 256, 256, 128],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_collector.collect(n_step=256 * 100)  # batch size * training_num\n",
        "\n",
        "res: 6/3\n",
        "\n",
        "3.\n",
        "\n",
        "net = Net(\n",
        "            state_shape=observation_space.shape or observation_space.n,\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[128, 256, 256, 128],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_collector.collect(n_step=128 * 100)  # batch size * training_num     \n",
        "\n",
        "{'bastaushy': 537, 'qostaushy': 411}\n",
        "\n",
        "4.\n",
        "\n",
        "net = Net(\n",
        "            state_shape=observation_space.shape or observation_space.n,\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[256, 512, 512, 256],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_collector.collect(n_step=128 * 100)  # batch size * training_num   \n",
        "\n",
        "res: {'bastaushy': 493, 'qostaushy': 483}\n",
        "\n",
        "res: {'bastaushy': 512, 'qostaushy': 464}\n",
        "\n",
        "5.\n",
        "\n",
        "net = Net(\n",
        "            state_shape=observation_space.shape or observation_space.n,\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[512, 1024, 1024, 512],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "res: {'bastaushy': 38, 'qostaushy': 59}\n",
        "\n",
        "res: {'bastaushy': 372, 'qostaushy': 562}"
      ],
      "metadata": {
        "id": "CqQs-aqbJmxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ¦Ž Play with different policies"
      ],
      "metadata": {
        "id": "VrclkOUXAe8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = PettingZooEnv(env())\n",
        "net1 = Net(\n",
        "            state_shape=(22,),\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[128, 256, 256, 128],\n",
        "            #hidden_sizes=[1024, 2048, 2048, 1024],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "agent1 = DQNPolicy(\n",
        "            model=net1,\n",
        "            optim = torch.optim.Adam(net1.parameters(), lr=1e-4),\n",
        "            discount_factor=0.9,\n",
        "            estimation_step=3,\n",
        "            target_update_freq=320,\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "agent1_learned = deepcopy(agent1)\n",
        "agent1_learned.load_state_dict(torch.load(agent1_path))\n",
        "\n",
        "net2 = Net(\n",
        "            state_shape=(22,),\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[256, 512, 512, 256],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "agent2 = DQNPolicy(\n",
        "            model=net2,\n",
        "            optim = torch.optim.Adam(net1.parameters(), lr=1e-4),\n",
        "            discount_factor=0.9,\n",
        "            estimation_step=3,\n",
        "            target_update_freq=320,\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "agent2_learned = deepcopy(agent2)\n",
        "agent2_learned.load_state_dict(torch.load(agent2_path))\n",
        "\n",
        "net3 = Net(\n",
        "            state_shape=(22,),\n",
        "            action_shape=env.action_space.shape or env.action_space.n,\n",
        "            hidden_sizes=[512, 1024, 1024, 512],\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "agent3 = DQNPolicy(\n",
        "            model=net3,\n",
        "            optim = torch.optim.Adam(net3.parameters(), lr=1e-4),\n",
        "            discount_factor=0.9,\n",
        "            estimation_step=3,\n",
        "            target_update_freq=320,\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "agent3_learned = deepcopy(agent3)\n",
        "agent3_learned.load_state_dict(torch.load(agent3_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL2WPtc0c0tx",
        "outputId": "87ee2241-8a6c-4027-890e-6b2c63362345"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PLAYS = {\"bastaushy\": 0, \"qostaushy\": 0}\n",
        "# Step 1: Load the PettingZoo environment\n",
        "env = env()#render_mode=\"human\")\n",
        "\n",
        "# Step 2: Wrap the environment for Tianshou interfacing\n",
        "env = PettingZooEnv(env)\n",
        "\n",
        "# # Step 3: Define policies for each agent\n",
        "policies = MultiAgentPolicyManager([agent1_learned, agent2_learned], env)\n",
        "# # Step 4: Convert the env to vector format\n",
        "env = DummyVectorEnv([lambda: env])\n",
        "\n",
        "# # Step 5: Construct the Collector, which interfaces the policies with the vectorised environment\n",
        "collector = Collector(policies, env)\n",
        "\n",
        "# # Step 6: Execute the environment with the agents playing for 1 episode, and render a frame every ? seconds\n",
        "result = collector.collect(n_episode=100)\n",
        "print(PLAYS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHioGuILePHq",
        "outputId": "a5cf65bd-0522-43d8-d4c9-a65092b6174b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bastaushy': 100, 'qostaushy': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¯ Experiment results"
      ],
      "metadata": {
        "id": "C2EnuhRNiUGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   agent3 vs agent1: 0-0\n",
        "*   agent1 vs agent3: 0-100\n",
        "*   agent3 vs agent2: 0-100\n",
        "*   agent2 vs agent3: 100-0\n",
        "*   agent2 vs agent1: 0-100\n",
        "*   agent1 vs agent2: 100-0\n",
        "\n"
      ],
      "metadata": {
        "id": "qEIOFq8YibxN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NutxrE0hgRkx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}